---
title: CLI
---

Discover and run benchmarks defined with `@bench` or `Bench`.

## Basic

```bash
pybench <files|directories> [options]
# alias
pyb <...>
```

- Directories expand to `**/*bench.py`.
- Colors enabled on TTY; use `--no-color` to disable.

## Options

```text
-k <word>             # filter by substring in case/file name
-P k=v                # override params: n, repeat, warmup, group and custom params (repeatable)
--no-color            # disable ANSI colors
--sort [group|time]   # sort groups or by avg time within group
--desc                # descending order
--budget 300ms        # per-variant budget (calibration). Accepts ns, ms, s
--max-n 1000000       # cap for calibrated n
--profile [thorough|smoke]
--brief               # compact output: benchmark, time(avg), vs base
--save LABEL          # save this run under .pybenchx/runs with optional label
--save-baseline NAME  # save this run as a named baseline under .pybenchx/baselines
--compare BASE|PATH   # compare current run against a baseline name or JSON file
--fail-on POLICY      # e.g. "mean:7%,p99:12%"; checked when --compare is used
--export FMT[:PATH]   # export as json|md|csv to stdout or a file when a path is given
```

Profiles:
- thorough: `budget≈1s`, `repeat=30`
- smoke (default): no calibration, `repeat=3`, `warmup=0`

## Overrides with -P

```bash
pybench examples/ -P repeat=5 -P n=10000 -P warmup=0
pybench examples/ -P group=strings
pybench examples/ -P sep=":"  # override a case parameter
```

## Sorting and brief mode

```bash
pybench examples/ --sort time --desc
pybench examples/ --brief
```

## Header and table

- Header: CPU, Python, clock resolution, profile.
- Table: mean, iter/s, min…max, p75/p99/p995, baseline and speed vs base. “≈ same” when ≤1% diff.

## Run management

- Runs and baselines are stored under `.pybenchx/` in your project root.
- `--compare` prints a simple diff and applies `--fail-on` thresholds.
- P-values via Mann–Whitney U (approx). `p99` policy uses actual P99 delta.

## Use in PRs and CI

- Generate a Markdown table and paste it into your PR description or a comment:

```bash
pybench path/ --export md:bench.md
# paste the contents of bench.md into your PR
```

- Automate with GitHub Actions (example):

```yaml
name: Benchmarks
on:
  pull_request:
    branches: [ main ]
jobs:
  bench:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install pybenchx
      - run: pybench examples/ --profile smoke --export md:bench.md
      - name: Post comment
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          body=$(printf "#### PyBenchX\n\n%s" "$(cat bench.md)")
          gh api repos/${{ github.repository }}/issues/${{ github.event.pull_request.number }}/comments \
            -f body="$body"
```

Tip: use `--profile smoke` for speed while iterating; switch to `thorough` on publishing runs.
